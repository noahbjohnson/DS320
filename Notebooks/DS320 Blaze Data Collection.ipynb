{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS320 Final Project\n",
    "## The Blaze Data Collection Notebook\n",
    "### Noah B Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib3\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from tinydb import TinyDB, Query\n",
    "import feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base url for scraping\n",
    "baseUrl = \"https://www.theblaze.com\"\n",
    "# url list name\n",
    "urlListName = \"urls.txt\"\n",
    "# scraping result json file name\n",
    "scrapedDataName = \"scraped.json\"\n",
    "# output json file name\n",
    "outputFileName = \"out.json\"\n",
    "# setup json file for tinydb\n",
    "db = TinyDB('db.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build List of Articles to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticles(response):\n",
    "    # Gets all the links to articles in the html response\n",
    "    # Returns a set\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    scrapeUrls = set()\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            if a[\"href\"].find(\"/news/\") > -1:\n",
    "                if a[\"href\"].find(baseUrl) > -1:\n",
    "                    scrapeUrls.add(a[\"href\"])\n",
    "                else:\n",
    "                    scrapeUrls.add(baseUrl + a[\"href\"])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return scrapeUrls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArticleList(daysBack):\n",
    "    # Build a set of links for articles on theblaze.com\n",
    "    # daysBack = How many days of data to collect\n",
    "    \n",
    "    # Get the articles on the front page\n",
    "    startPage = requests.get(\"https://www.theblaze.com/\")\n",
    "    articleList = getArticles(startPage)\n",
    "    \n",
    "    # The number of archive pages (days) scraped\n",
    "    pages = 0\n",
    "    \n",
    "    # Set the iterative variables for the url\n",
    "    day = 16\n",
    "    month = 11\n",
    "    year = 2018\n",
    "    \n",
    "    # Go through the archive pages and add article links to the set\n",
    "    while pages < daysBack:\n",
    "        # Create archive url based on date iterators\n",
    "        pageLink = \"https://www.theblaze.com/news/{}/{}/{}\".format(year,month,day)\n",
    "        # Get the page html\n",
    "        page = requests.get(pageLink)\n",
    "        # Get the article links from the response html\n",
    "        articleList = articleList.union(getArticles(page))\n",
    "        \n",
    "        # Take the date down by one\n",
    "        day = day - 1\n",
    "        if day == 0:\n",
    "            month = month - 1\n",
    "            day = 31\n",
    "        if month == 0:\n",
    "            year = year - 1\n",
    "            month = 12\n",
    "            \n",
    "        # Update the pages scraped value\n",
    "        pages +=1\n",
    "            \n",
    "        \n",
    "    return articleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the link generation function\n",
    "\n",
    "# Load from file after first run!!\n",
    "# urlList = buildArticleList(1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the urls to a file\n",
    "\n",
    "# Load from file after first run!!\n",
    "# article_urls = open(urlListName,'w')\n",
    "# for line in mylist:\n",
    "#     article_urls.write(line)\n",
    "#     article_urls.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the urls from the file\n",
    "\n",
    "# Open the File\n",
    "urls = open(\"urls.txt\",'r').readlines()\n",
    "\n",
    "# remove newlines from the urls\n",
    "for i in range(len(urls)):\n",
    "    urls[i] = urls[i].replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape List of Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseArticle(response):\n",
    "    # Scrapes an article html response for title, author, body, and time\n",
    "    # Try/Catch block returns none if not all the fields are found\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        time = soup.find('time')[\"datetime\"]\n",
    "        author = soup.find('span',{'class':'author-name'}).text\n",
    "        title = soup.find(\"h1\", {\"class\": \"page-title\"}).text\n",
    "        body = ''\n",
    "        for p in soup.find('div', {'class': 'entry-content article-styles'}).find_all(['p','h3','ul']):\n",
    "            if p.text.find(\"H/T: \") == -1:\n",
    "                body = body + p.text  + '\\n'\n",
    "    except:\n",
    "        return None\n",
    "    return {\n",
    "        'url': response.url,\n",
    "        'time': time,\n",
    "        'author': author,\n",
    "        'title': title,\n",
    "        'body': body,\n",
    "        'site': 'theblaze'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeDB(lines):\n",
    "    # Takes list of urls and scrapes them to a tinydb\n",
    "    \n",
    "    # Count variable for printing progress\n",
    "    count = 0\n",
    "    \n",
    "    # query object for checking urls\n",
    "    q = Query()\n",
    "    \n",
    "    # Scrape!\n",
    "    for line in lines:\n",
    "        # Sleep to prevent https issue or getting ip blocked for ddos\n",
    "        time.sleep(.1)\n",
    "        # print progress every 50 articles\n",
    "        if count % 50 == 0:\n",
    "            print(str(count / len(lines) * 100) + \"%\")\n",
    "#         print(\"db length: \" + str(len(db)))\n",
    "        # get the page\n",
    "        if not db.contains(q.url == line):\n",
    "            try:\n",
    "                response = requests.get(line)\n",
    "                # scrape the data from the page and add it to the df\n",
    "                db.insert(parseArticle(response))\n",
    "            except ValueError:\n",
    "#                 print(\"failure on \" + line)\n",
    "                pass\n",
    "        # add one to the count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 4.5k urls from the 14505 in the list\n",
    "# scrapeDB(random.sample(urls, 4500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
